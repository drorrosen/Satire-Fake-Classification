{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pattern.en import sentiment\n",
    "from pattern.en import parse, Sentence\n",
    "from pattern.en import modality, mood\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "#modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "\n",
    "#saving model \n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "FAKE_SOURCE_DIR = 'StoryText 2/Fake/finalFake'\n",
    "SATIRE_SOURCE_DIR = 'StoryText 2/Satire/finalSatire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score is 0.7213114754098361]\n",
      "\n",
      "confusion_matrix:\n",
      " [[51 21]\n",
      " [13 37]]\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Fake       0.80      0.71      0.75        72\n",
      "     Satire       0.64      0.74      0.69        50\n",
      "\n",
      "avg / total       0.73      0.72      0.72       122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pattern.en import sentiment\n",
    "from pattern.en import parse, Sentence\n",
    "from pattern.en import modality, mood\n",
    "import spacy\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "#modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#saving model \n",
    "import joblib\n",
    "\n",
    "\n",
    "#paths\n",
    "FAKE_SOURCE_DIR = 'StoryText 2/Fake/finalFake'\n",
    "SATIRE_SOURCE_DIR = 'StoryText 2/Satire/finalSatire'\n",
    "\n",
    "\n",
    "class CleaningDf:\n",
    "    \"\"\"\n",
    "    Cleaning the dataset and merging the text documents to the dataset.\n",
    "    Returns a clean version of the data\n",
    "    \"\"\"\n",
    "    def __init__(self,path = 'Fake News Stories.xlsx', target_col='Fake or Satire?', text_col='Text'):\n",
    "        \"\"\"\n",
    "        :param df: The dataset with the Text and target columns\n",
    "        :param target_col: the target column\n",
    "        :param text_col: the text column\n",
    "        \"\"\"\n",
    "        self.df = pd.read_excel(path)\n",
    "        self.target_col = target_col\n",
    "        self.text_col = text_col\n",
    "        \n",
    "    def extract_texts(self):\n",
    "        \"\"\"\n",
    "        Read the text articles and create a temporary dataframe.\n",
    "        returns a dataframe with two columns: Article Number and Text.\n",
    "        \"\"\"\n",
    "        text_dict = {'Article Number': [] , 'Text': []}\n",
    "        \n",
    "        for folder in [FAKE_SOURCE_DIR, SATIRE_SOURCE_DIR]:\n",
    "            for text in os.listdir(folder):\n",
    "                if text.endswith('.txt'):\n",
    "                    article_num = int(text.split('.txt')[0])\n",
    "                    full_path = os.path.join(folder,text)\n",
    "                    with open(full_path, encoding='Latin-1') as txt:\n",
    "                        text_dict['Text'].append(txt.read())\n",
    "                        text_dict['Article Number'].append(article_num)\n",
    "                        \n",
    "                        \n",
    "        return pd.DataFrame(text_dict)\n",
    "\n",
    "    def merge_dfs(self):\n",
    "        \"\"\"\n",
    "        Merging the original dataframe  with the dataframe with the text, on Article Number.\n",
    "        \"\"\"\n",
    "        self.df = pd.merge(left=self.df, right=self.extract_texts(), on='Article Number')\n",
    "        self.df.loc[self.df['Fake or Satire?'] == 'Satire ', 'Fake or Satire?'] = 'Satire'\n",
    "        self.df = self.df[['Text','Fake or Satire?', 'URL of rebutting article']]\n",
    "\n",
    "        return self.df[[self.text_col, self.target_col]]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Preprocessing:\n",
    "    \"\"\"\n",
    "    Doing data Engineering and split to train and test.\n",
    "    \"\"\"\n",
    "    def __init__(self,df, test_size=0.25, dim_reduction = True, n_components=100):\n",
    "        \"\"\"\n",
    "        :param df: The cleaned dataframe\n",
    "        :param test_size: The size of the test dataset\n",
    "        :param dim_reduction: use PCA or not.\n",
    "        :param n_components: number of components to choose for PCA\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.test_size= 0.25\n",
    "        self.dim_reduction = dim_reduction\n",
    "        self.n_components = 100\n",
    "    \n",
    "    \n",
    "                \n",
    "        \n",
    "    def adding_bert_features(self):\n",
    "        \"\"\"\n",
    "        BERT Embeddings on the text column.\n",
    "        \"\"\"\n",
    "        model = SentenceTransformer('stsb-bert-base')\n",
    "        embbed_text = self.df['Text'].apply(lambda x: model.encode(x))\n",
    "        embbed_text = np.array(embbed_text.tolist())\n",
    "        embbed_df = pd.DataFrame(embbed_text)\n",
    "        self.df = pd.concat([self.df, embbed_df],axis=1)\n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "    \n",
    "    def _create_datasets(self):\n",
    "        \"\"\"\n",
    "        creates train and test datasets.\n",
    "        :return: train and test datasets\n",
    "        \n",
    "        \"\"\"\n",
    "        X = self.df.drop(columns='Fake or Satire?')\n",
    "        y = self.df['Fake or Satire?']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size)\n",
    "        \n",
    "        self.X_train = X_train.reset_index(drop=True)\n",
    "        self.X_test = X_test.reset_index(drop=True)\n",
    "        self.y_train = y_train.reset_index(drop=True)\n",
    "        self.y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    def feature_engineering(self):\n",
    "        \"\"\"\n",
    "        Adding new features to the datasets.\n",
    "        Returns training and test datasets after feature engineering.\n",
    "        \"\"\"\n",
    "            \n",
    "\n",
    "        #clean text\n",
    "        self.X_train['Text'] = self.X_train['Text'].apply(lambda x: self.get_clean_text(x))\n",
    "        self.X_test['Text'] = self.X_test['Text'].apply(lambda x: self.get_clean_text(x))\n",
    "\n",
    "        #add number of words \n",
    "        self.X_train['word_num'] = self.X_train['Text'].apply(lambda s: len(s.split()))\n",
    "        self.X_test['word_num'] = self.X_test['Text'].apply(lambda s: len(s.split()))\n",
    "        \n",
    "        \n",
    "        #adding modality and sentiment\n",
    "        self.X_train = self.adding_sentiment(self.X_train)\n",
    "        self.X_test = self.adding_sentiment(self.X_test)\n",
    "\n",
    "        #drop URL of rebutting article column\n",
    "        self.X_train.drop(columns=['Text'], inplace=True)\n",
    "        self.X_test.drop(columns=['Text'], inplace=True)\n",
    "        \n",
    "        if self.dim_reduction:\n",
    "            pca = PCA(n_components=self.n_components)\n",
    "            self.X_train = pca.fit_transform(self.X_train)\n",
    "            self.X_test = pca.transform(self.X_test)\n",
    "            \n",
    "            #saving pca\n",
    "            joblib.dump(pca, 'PCA.pkl')\n",
    "            \n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "            \n",
    "                        \n",
    "\n",
    "    def get_clean_text(self, row):\n",
    "        \"\"\"\n",
    "        Some basic NLP preprocessing - tokenization and removing stopwords.\n",
    "        \"\"\"\n",
    "        #lower letters\n",
    "        row = row.lower()\n",
    "        #removing stopwords\n",
    "        row = ' '.join([word for word in row.split() if word not in nlp.Defaults.stop_words])\n",
    "        #tokenization\n",
    "        doc = nlp(row)\n",
    "        row = ' '.join([token.text for token in doc])\n",
    "        \n",
    "        return row\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    def adding_sentiment(self, data):\n",
    "        \"\"\"\n",
    "        Adding sentiment, mood and modality as new features.\n",
    "        \"\"\"\n",
    "\n",
    "        data['polarity'], data['subjectivity'] = data['Text'].apply(lambda x: sentiment(x)).str\n",
    "        data['mood'], data['modality'] = data['Text'].apply(lambda x: self.adding_mood_modality(x)).str\n",
    "\n",
    "\n",
    "        hot_encoded = pd.get_dummies(data['modality'])\n",
    "        data = pd.concat([data.drop(columns='modality'), hot_encoded], sort=False, axis=1)\n",
    "\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    def adding_mood_modality(self, row):\n",
    "        \"\"\"\n",
    "        Calculating mood and modality from a text/row\n",
    "        \"\"\"\n",
    "        row = parse(row, lemmata=True)\n",
    "        row = Sentence(row)\n",
    "\n",
    "        return modality(row), mood(row)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Training the model and saving it as .pkl.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        \"\"\"\n",
    "        :param X_train: X_train dataset\n",
    "        :param X_test: X_test dataset\n",
    "        :param y_train: target train column\n",
    "        :param y_test: target test column\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.model = LogisticRegression()\n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Doing cross validation to find the best hyperparameters \n",
    "        for the model and then fitting the model on the train dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create first pipeline for base without reducing features.\n",
    "\n",
    "        pipe = Pipeline([('classifier' , self.model)])\n",
    "\n",
    "        # Create param grid.\n",
    "\n",
    "        param_grid = [\n",
    "            {'classifier' : [self.model],\n",
    "             'classifier__penalty' : ['l1', 'l2'],\n",
    "            'classifier__solver' : ['liblinear']}]\n",
    "\n",
    "        # Create grid search object\n",
    "\n",
    "        clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5)\n",
    "        \n",
    "        self.model = clf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        \n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Predicting the test dataset based on the fitted model.\n",
    "        \"\"\"\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        return self.y_pred\n",
    "        \n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        model evaluation.\n",
    "        \"\"\"\n",
    "        print(f'accuracy_score is {accuracy_score(self.y_test, self.y_pred)}]\\n')\n",
    "        print(f'confusion_matrix:\\n {confusion_matrix(self.y_test, self.y_pred)}')\n",
    "        print(f'classification report:\\n {classification_report(self.y_test, self.y_pred)}')\n",
    "    \n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Saving model\n",
    "        \"\"\"\n",
    "        joblib.dump(self.model, 'model_Fake_Satire.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #Cleaning data\n",
    "    df = CleaningDf()\n",
    "    df = df.merge_dfs()\n",
    "    \n",
    "    #initiating the class preprocessing\n",
    "    pre = Preprocessing(df)\n",
    "    #adding embeddings\n",
    "    pre.adding_bert_features()\n",
    "    #splitting data\n",
    "    pre._create_datasets()\n",
    "    #feature engineering \n",
    "    X_train, X_test, y_train, y_test = pre.feature_engineering()\n",
    "    #Training model\n",
    "    \n",
    "    model = Model(X_train, X_test, y_train, y_test)\n",
    "    model.fit()\n",
    "    model.predict()\n",
    "    #evauluation\n",
    "    model.evaluate()\n",
    "    \n",
    "    #save the model to the local machine\n",
    "    model.save_model()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
